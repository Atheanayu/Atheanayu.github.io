<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>Han</title><meta name="author" content="Han Yu"><meta name="copyright" content="Han Yu"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta property="og:type" content="website">
<meta property="og:title" content="Han">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="Han">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png">
<meta property="article:author" content="Han Yu">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Han',
  isPost: false,
  isHome: true,
  isHighlightShrink: false,
  isToc: false,
  postUpdate: '2022-01-20 22:54:38'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.0.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data is-center"><div class="data-item"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">10</div></a></div><div class="data-item"><a href="/tags/"><div class="headline">Tags</div><div class="length-num">3</div></a></div><div class="data-item"><a href="/categories/"><div class="headline">Categories</div><div class="length-num">4</div></a></div></div><hr/></div></div><div class="page" id="body-wrap"><header class="full_page" id="page-header" style="background-image: url('https://s2.loli.net/2022/01/07/WHBD4aVG6fcXmxj.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Han</a></span><div id="menus"><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="site-info"><h1 id="site-title">Han</h1></div><div id="scroll-down"><i class="fas fa-angle-down scroll-down-effects"></i></div></header><main class="layout" id="content-inner"><div class="recent-posts" id="recent-posts"><div class="recent-post-item"><div class="post_cover left"><a href="/2022/01/20/%E5%85%8D%E6%A8%A1%E5%9E%8B%E6%8E%A7%E5%88%B6/" title="免模型控制"><img class="post_bg" src="https://s2.loli.net/2022/01/07/WHBD4aVG6fcXmxj.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="免模型控制"></a></div><div class="recent-post-info"><a class="article-title" href="/2022/01/20/%E5%85%8D%E6%A8%A1%E5%9E%8B%E6%8E%A7%E5%88%B6/" title="免模型控制">免模型控制</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2022-01-20T14:47:48.000Z" title="Created 2022-01-20 22:47:48">2022-01-20</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-David-Silver/">强化学习-David Silver</a></span></div><div class="content">

免模型控制

Generalised Policy Iteration With Monte-Carlo Evaluation (On-Policy)

ε\varepsilonε-greedy exploration
GLIE 蒙特卡罗控制


TD Control (On-Policy)

Sarsa


Off-Policy Learning
总结




免模型控制
免模型控制可以认为是前面介绍的免模型预测的延续。
免模型控制按照环境交互的策略和目标策略是否相同可以将方法分为两种：

On-policy: 环境交互策略和目标策略一致。就像古代皇帝微服私访，亲力亲为。
Off-policy: 环境交互策略和目标策略不同。就像古代皇帝，派多个官员了解民情。或者机器观察人如何和环境交互，从而学习，它不直接从环境中提取经验，而是从与环境交互的智能体提取经验。。

Generalised Policy Iteration With Monte-Carlo Evaluation (On-Policy)
策略迭代之前在动态规划的控制部分就介绍过。但是它是基于MDP的一种方法，也就是它是一 ...</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/2022/01/19/%E5%85%8D%E6%A8%A1%E5%9E%8B%E9%A2%84%E6%B5%8B/" title="免模型预测"><img class="post_bg" src="https://s2.loli.net/2022/01/07/WHBD4aVG6fcXmxj.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="免模型预测"></a></div><div class="recent-post-info"><a class="article-title" href="/2022/01/19/%E5%85%8D%E6%A8%A1%E5%9E%8B%E9%A2%84%E6%B5%8B/" title="免模型预测">免模型预测</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2022-01-19T09:40:16.000Z" title="Created 2022-01-19 17:40:16">2022-01-19</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-David-Silver/">强化学习-David Silver</a></span></div><div class="content">

免模型预测

蒙特卡罗学习

First-Visit Monte-Carlo Policy Evaluation
Every-Visit Monte-Carlo Policy Evaluation
增量形式的蒙特卡罗更新


时序差分学习

TD(0)
TD(λ\lambdaλ)


蒙特卡罗 vs 时序差分




免模型预测
前面介绍的方法假定我们已知状态转移函数和奖励函数，这在许多游戏的场景中是可行的。但是仍然存在部分场景，我们不能了解它的状态转移函数和奖励函数。
蒙特卡罗学习
蒙特卡罗方法是一种免模型的方法，也就是它不需要环境的状态转移函数以及环境的奖励函数。
蒙特卡罗方法利用完整的回合经验来学习，也就是利用采样样本学习。它的核心思想是利用均值估计期望。
由于这是一个预测任务，因此学习目标是学习给定策略的v函数。
我们之前定义的return和状态价值函数：
Gt=Rt+1+γRt+2+γ2Rt+3+...vπ(s)=Eπ[Gt∣St=s]\begin{equation}
\begin{aligned}
G_t &amp;= R_{t + 1} + \gamma R_{t + ...</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/2022/01/18/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/" title="动态规划"><img class="post_bg" src="https://s2.loli.net/2022/01/07/WHBD4aVG6fcXmxj.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="动态规划"></a></div><div class="recent-post-info"><a class="article-title" href="/2022/01/18/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/" title="动态规划">动态规划</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2022-01-18T14:02:36.000Z" title="Created 2022-01-18 22:02:36">2022-01-18</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-David-Silver/">强化学习-David Silver</a></span></div><div class="content">

动态规划

预测

同步备份
异步备份


控制

策略迭代
值迭代
策略迭代 vs 值迭代


总结
技巧




动态规划
动态表示的是一种问题有时间上的特性，问题一步一步的改变。
动态规划的基本思想是将一个大的问题分解成小的问题，再将小问题的答案结合起来得到最终结果。
动态规划用于解决具有以下两个性质的问题：

具有最优化子结构：可以将问题分解成小问题，分别解决从而解决最终问题
相同的子问题会多次出现，结果可以复用

马尔可夫决策过程（MDP）满足以上两个性质。

贝尔曼方程将问题递归的分解成子问题
价值函数存储和复用子问题的结论，从而获得最终价值函数

动态规划假定可以了解完整的环境信息，也就是假定它知道状态转移概率和奖励函数。也就是动态规划解决的是：如果我们知道状态转移概率和奖励函数，那么我们该如何找到状态价值函数和最优策略
动态规划的过程可以分为两部分：

预测：评估，也就是根据已知策略获得价值函数的过程

输入：

MDP &lt;S,A,P,R,γ&gt;&lt;S, A, P, R, \gamma&gt;&lt;S,A,P,R,γ&gt; 和策略 π\piπ
MR ...</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/2022/01/17/%E6%9C%89%E9%99%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/" title="有限马尔可夫决策过程"><img class="post_bg" src="https://s2.loli.net/2022/01/07/WHBD4aVG6fcXmxj.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="有限马尔可夫决策过程"></a></div><div class="recent-post-info"><a class="article-title" href="/2022/01/17/%E6%9C%89%E9%99%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/" title="有限马尔可夫决策过程">有限马尔可夫决策过程</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2022-01-17T15:17:28.000Z" title="Created 2022-01-17 23:17:28">2022-01-17</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-David-Silver/">强化学习-David Silver</a></span></div><div class="content">

马尔可夫决策过程

马尔可夫决策过程（MDP）的介绍

马尔可夫过程
马尔可夫奖励过程
马尔可夫决策过程






马尔可夫决策过程
一个Agent包含3部分：

策略：action，是agent的behavior。目标
价值函数：评估。对未来做出估计，找到更好的策略
模型：agent对env的表示。根据模型做出计划。包含两部分：

转移概率
reward函数



根据有无价值函数可以将方法分为：策略梯度方法，基于价值函数的方法，actor-critic
根据有无模型可以将方法分为：有模型和无模型
预测：评估策略的价值
控制：寻找最优策略
马尔可夫决策过程（MDP）的介绍
MDP假设环境状态完全可观测。但是部分可观测问题可以转换成为MDP问题来解决。
马尔可夫过程
马尔可夫是一个无记忆的随机序列。它的状态满足马尔可夫性质。马尔可夫过程可以表示为&lt;S,P&gt;&lt;S, P&gt;&lt;S,P&gt;. S为状态集合，P为转移概率。
马尔可夫奖励过程
马尔可夫奖励过程是带有reward的马尔可夫过程。可以表示为&lt;S,P,R,γ&gt;&lt;S, P, R, \ ...</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/2022/01/17/%E6%A6%82%E8%A7%88/" title="概览"><img class="post_bg" src="https://s2.loli.net/2022/01/07/WHBD4aVG6fcXmxj.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="概览"></a></div><div class="recent-post-info"><a class="article-title" href="/2022/01/17/%E6%A6%82%E8%A7%88/" title="概览">概览</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2022-01-17T03:47:25.000Z" title="Created 2022-01-17 11:47:25">2022-01-17</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%A6%82%E8%A7%88/">概览</a></span></div><div class="content">EasyRL: 停止更新。
</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/2022/01/17/%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6/" title="策略梯度"><img class="post_bg" src="https://s2.loli.net/2022/01/07/WHBD4aVG6fcXmxj.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="策略梯度"></a></div><div class="recent-post-info"><a class="article-title" href="/2022/01/17/%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6/" title="策略梯度">策略梯度</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2022-01-16T16:18:04.000Z" title="Created 2022-01-17 00:18:04">2022-01-17</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/EasyRL/">EasyRL</a></span></div><div class="content">

策略梯度

技巧

添加基线
分配合适的分数


REINFORCE：蒙特卡罗策略




策略梯度
强化学习有3个组成部分：演员，环境，奖励
一轮游戏可以叫做一个回合（episode）或者试验（trial）
游戏最终的总奖励（total reward）
一个完整的轨迹可以写成：T={s1,a1,s2,a2,...,st,at}T = \{s_1, a_1, s_2, a_2, ..., s_t, a_t\}T={s1​,a1​,s2​,a2​,...,st​,at​}
该轨迹的概率：
pθ(τ)=p(s1)pθ(a1∣s1)p(s2∣s1,a1)pθ(a2∣s2)...=p(s1)∏t=1Tpθ(at∣st)p(st+1∣st,at)\begin{equation}
\begin{aligned}
p_{\theta}(\tau) 
&amp;= p(s_1)p_{\theta}(a_1|s_1)p(s_2|s_1, a_1)p_{\theta}(a_2|s_2)...\\
&amp;= p(s_1)\prod\limits_{t = 1}^T p_{\theta}(a_t|s_ ...</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/2022/01/16/Q%E8%A1%A8%E6%A0%BC/" title="Q表格"><img class="post_bg" src="https://s2.loli.net/2022/01/07/WHBD4aVG6fcXmxj.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Q表格"></a></div><div class="recent-post-info"><a class="article-title" href="/2022/01/16/Q%E8%A1%A8%E6%A0%BC/" title="Q表格">Q表格</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2022-01-15T16:52:24.000Z" title="Created 2022-01-16 00:52:24">2022-01-16</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/EasyRL/">EasyRL</a></span></div><div class="content">

Q表格

免模型预测

蒙特卡罗策略评估
时序差分
动态规划、蒙特卡罗以及时序差分


免模型控制

探索性初始化蒙特卡罗法
ε\varepsilonε-贪心蒙特卡罗算法
Sarsa：同策略时序差分控制
Q-learning：异策略时序差分控制
同策略 vs 异策略


总结




Q表格
上一章的马尔可夫决策过程说的是有模型情况下的预测和控制。这一章节的Q表格则是介绍了免模型情况下的预测和控制方法。
免模型预测
免模型说的是，不对环境的转移概率和奖励函数进行学习。
蒙特卡罗策略评估
上一章也简单介绍了蒙特卡罗策略评估的方法。简单来说就是对轨迹采样取平均作为期望值。下面会具体介绍。
一条轨迹的回报：
Gt=Rt+1+γRt+2+γ2Rt+3+...G_t = R_{t + 1} + \gamma R_{t + 2} + \gamma^2 R_{t + 3} + ...
Gt​=Rt+1​+γRt+2​+γ2Rt+3​+...
状态价值是s状态以后的轨迹回报的期望：
vT(s)=Eτ∼π[Gt∣st=s]v^T(s) = E_{\tau \sim \pi}[G_t|s_t = s] ...</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/2022/01/14/%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/" title="马尔可夫决策过程"><img class="post_bg" src="https://s2.loli.net/2022/01/07/WHBD4aVG6fcXmxj.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="马尔可夫决策过程"></a></div><div class="recent-post-info"><a class="article-title" href="/2022/01/14/%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/" title="马尔可夫决策过程">马尔可夫决策过程</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2022-01-13T17:44:00.000Z" title="Created 2022-01-14 01:44:00">2022-01-14</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/EasyRL/">EasyRL</a></span></div><div class="content">

马尔可夫决策过程

马尔可夫过程
马尔可夫奖励过程

回报与价值函数
V函数的贝尔曼方程
计算马尔可夫奖励过程价值的迭代算法


马尔可夫决策过程

马尔可夫决策过程中的策略
马尔可夫决策过程中的价值函数
Q函数的贝尔曼方程
备份图
预测与控制
动态规划解决马尔可夫决策过程
马尔可夫决策过程的策略评估
马尔可夫决策过程控制






概念：
范围（horizon）：一个回合的长度
回报（return）：奖励折扣后获得的收益
状态价值函数：回报的期望
马尔可夫决策过程
强化学习的过程可以用马尔可夫过程来描述，因为当前状态只和前一个状态以及采取的action有关。马尔可夫决策过程 是强化学习的一个基本框架。
首先介绍马尔可夫奖励过程，它是马尔可夫决策过程的一个简化版本。
接下来会介绍马尔可夫决策过程中的策略评估，给定一个决策，怎么计算它的价值。
最后会介绍马尔可夫决策过程中的决策控制，包含策略迭代和价值迭代两种方式。
马尔可夫过程
马尔可夫性：
当前状态只依赖于前一个状态。
马尔可夫过程/马尔可夫链
可以用状态转移矩阵描述，略。
马尔可夫奖励过程
Markov reward proc ...</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/2022/01/12/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0/" title="强化学习概述"><img class="post_bg" src="https://s2.loli.net/2022/01/07/WHBD4aVG6fcXmxj.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="强化学习概述"></a></div><div class="recent-post-info"><a class="article-title" href="/2022/01/12/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0/" title="强化学习概述">强化学习概述</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2022-01-12T06:41:47.000Z" title="Created 2022-01-12 14:41:47">2022-01-12</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/EasyRL/">EasyRL</a></span></div><div class="content">

强化学习概述

强化学习

强化学习与监督学习


序列决策介绍

奖励
序列决策


动作空间
强化学习智能体的组成成分和类型

策略
价值函数
模型
举例


强化学习智能体类型

价值 vs 策略
有模型 vs 免模型


学习与规划
探索和利用




EasyRL 阅读笔记
强化学习概述
强化学习
强化学习中存在两个角色：agent 和 env。agent 根据从env获得的observation（or state）作出action，env根据action给agent相应的reward，同时action也会对环境作出改变。agent再从env获得observation（由于之前的action改变了环境，导致产生了和之前不同的observation）。。
强化学习的一个重要优势在于，它可以超越人类。对于有监督学习来说，由于数据是人类标注的，因此模型的上限就是人类的表现，模型不能超越人类。但是强化学习的数据没有人类的标注，因此它有超越人类表现的可能。
强化学习与监督学习
监督学习的假设：

输入数据之间不存在关联。它假设数据之间是iid的
数据是有标签的，学习器可以通过标签来 ...</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/2022/01/07/RNN/" title="RNN"><img class="post_bg" src="//user-images.githubusercontent.com/15166794/39033684-3053546e-44ae-11e8-893a-7fa685039ce2.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="RNN"></a></div><div class="recent-post-info"><a class="article-title" href="/2022/01/07/RNN/" title="RNN">RNN</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2022-01-07T06:40:16.000Z" title="Created 2022-01-07 14:40:16">2022-01-07</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/BasicModels/">BasicModels</a></span></div><div class="content">

CNN
RNN
LSTM
VARIANTS ON LSTM

PEEPHOLE VERSION
SHARED GATE
GRU(待补充)


BILSTM
TRANSFORMER

ENCODER
SELF-ATTENTION
RESIDUALS
DECODER


Self Regression


CNN
特点：参数共享
引入卷积核的原因在于全连接层所需的参数太多容易过拟合
卷积的作用
​		卷积核对应图片之中的感受野，卷积上的权重表现了图片不同位置的focus。
​		sobel算子：[[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]] 可以检测水平梯度，也就是检测纵向边缘。而[[-1, -2, -1], [0, 0, 0], [1, 2, 1]] 可以检测纵向梯度，也就是横向边缘。
小卷积核 vs 大卷积核
​		多个小卷积核可以起到一个大卷积核的作用。比如2个3x3的卷积核和1个5x5的卷积核可以起到相同的作用。而2x9x9 &lt; 5 * 5 因此采用多个小卷积核可以起到减少参数，防止过拟合的作用。
Dropout
​		每次输入都有一定概率删除部 ...</div></div></div><nav id="pagination"><div class="pagination"><span class="page-number current">1</span></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Han Yu</div><div class="author-info__description"></div></div><div class="card-info-data is-center"><div class="card-info-data-item"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">10</div></a></div><div class="card-info-data-item"><a href="/tags/"><div class="headline">Tags</div><div class="length-num">3</div></a></div><div class="card-info-data-item"><a href="/categories/"><div class="headline">Categories</div><div class="length-num">4</div></a></div></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>Announcement</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2022/01/20/%E5%85%8D%E6%A8%A1%E5%9E%8B%E6%8E%A7%E5%88%B6/" title="免模型控制"><img src="https://s2.loli.net/2022/01/07/WHBD4aVG6fcXmxj.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="免模型控制"/></a><div class="content"><a class="title" href="/2022/01/20/%E5%85%8D%E6%A8%A1%E5%9E%8B%E6%8E%A7%E5%88%B6/" title="免模型控制">免模型控制</a><time datetime="2022-01-20T14:47:48.000Z" title="Created 2022-01-20 22:47:48">2022-01-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/01/19/%E5%85%8D%E6%A8%A1%E5%9E%8B%E9%A2%84%E6%B5%8B/" title="免模型预测"><img src="https://s2.loli.net/2022/01/07/WHBD4aVG6fcXmxj.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="免模型预测"/></a><div class="content"><a class="title" href="/2022/01/19/%E5%85%8D%E6%A8%A1%E5%9E%8B%E9%A2%84%E6%B5%8B/" title="免模型预测">免模型预测</a><time datetime="2022-01-19T09:40:16.000Z" title="Created 2022-01-19 17:40:16">2022-01-19</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/01/18/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/" title="动态规划"><img src="https://s2.loli.net/2022/01/07/WHBD4aVG6fcXmxj.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="动态规划"/></a><div class="content"><a class="title" href="/2022/01/18/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/" title="动态规划">动态规划</a><time datetime="2022-01-18T14:02:36.000Z" title="Created 2022-01-18 22:02:36">2022-01-18</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/01/17/%E6%9C%89%E9%99%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/" title="有限马尔可夫决策过程"><img src="https://s2.loli.net/2022/01/07/WHBD4aVG6fcXmxj.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="有限马尔可夫决策过程"/></a><div class="content"><a class="title" href="/2022/01/17/%E6%9C%89%E9%99%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/" title="有限马尔可夫决策过程">有限马尔可夫决策过程</a><time datetime="2022-01-17T15:17:28.000Z" title="Created 2022-01-17 23:17:28">2022-01-17</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/01/17/%E6%A6%82%E8%A7%88/" title="概览"><img src="https://s2.loli.net/2022/01/07/WHBD4aVG6fcXmxj.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="概览"/></a><div class="content"><a class="title" href="/2022/01/17/%E6%A6%82%E8%A7%88/" title="概览">概览</a><time datetime="2022-01-17T03:47:25.000Z" title="Created 2022-01-17 11:47:25">2022-01-17</time></div></div></div></div><div class="card-widget card-categories"><div class="item-headline">
            <i class="fas fa-folder-open"></i>
            <span>Categories</span>
            
            </div>
            <ul class="card-category-list" id="aside-cat-list">
            <li class="card-category-list-item "><a class="card-category-list-link" href="/categories/BasicModels/"><span class="card-category-list-name">BasicModels</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/EasyRL/"><span class="card-category-list-name">EasyRL</span><span class="card-category-list-count">4</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-David-Silver/"><span class="card-category-list-name">强化学习-David Silver</span><span class="card-category-list-count">4</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/%E6%A6%82%E8%A7%88/"><span class="card-category-list-name">概览</span><span class="card-category-list-count">1</span></a></li>
            </ul></div><div class="card-widget card-tags"><div class="item-headline"><i class="fas fa-tags"></i><span>Tags</span></div><div class="card-tag-cloud"><a href="/tags/basic/" style="font-size: 1.1em; color: #999">basic</a> <a href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" style="font-size: 1.5em; color: #99a9bf">强化学习</a> <a href="/tags/%E6%A6%82%E8%A7%88/" style="font-size: 1.1em; color: #999">概览</a></div></div><div class="card-widget card-archives"><div class="item-headline"><i class="fas fa-archive"></i><span>Archives</span></div><ul class="card-archive-list"><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2022/01/"><span class="card-archive-list-date">January 2022</span><span class="card-archive-list-count">10</span></a></li></ul></div><div class="card-widget card-webinfo"><div class="item-headline"><i class="fas fa-chart-line"></i><span>Info</span></div><div class="webinfo"><div class="webinfo-item"><div class="item-name">Article :</div><div class="item-count">10</div></div><div class="webinfo-item"><div class="item-name">UV :</div><div class="item-count" id="busuanzi_value_site_uv"></div></div><div class="webinfo-item"><div class="item-name">PV :</div><div class="item-count" id="busuanzi_value_site_pv"></div></div><div class="webinfo-item"><div class="item-name">Last Push :</div><div class="item-count" id="last-push-date" data-lastPushDate="2022-01-20T14:54:37.991Z"></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2022 By Han Yu</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>