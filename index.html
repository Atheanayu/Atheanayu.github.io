<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>Han</title><meta name="author" content="Han Yu"><meta name="copyright" content="Han Yu"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta property="og:type" content="website">
<meta property="og:title" content="Han">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="Han">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png">
<meta property="article:author" content="Han Yu">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Han',
  isPost: false,
  isHome: true,
  isHighlightShrink: false,
  isToc: false,
  postUpdate: '2022-01-23 15:57:46'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.0.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data is-center"><div class="data-item"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">12</div></a></div><div class="data-item"><a href="/tags/"><div class="headline">Tags</div><div class="length-num">3</div></a></div><div class="data-item"><a href="/categories/"><div class="headline">Categories</div><div class="length-num">4</div></a></div></div><hr/></div></div><div class="page" id="body-wrap"><header class="full_page" id="page-header" style="background-image: url('https://s2.loli.net/2022/01/07/WHBD4aVG6fcXmxj.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Han</a></span><div id="menus"><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="site-info"><h1 id="site-title">Han</h1></div><div id="scroll-down"><i class="fas fa-angle-down scroll-down-effects"></i></div></header><main class="layout" id="content-inner"><div class="recent-posts" id="recent-posts"><div class="recent-post-item"><div class="post_cover left"><a href="/2022/01/23/%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E7%AE%97%E6%B3%95/" title="策略梯度算法"><img class="post_bg" src="https://s2.loli.net/2022/01/07/WHBD4aVG6fcXmxj.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="策略梯度算法"></a></div><div class="recent-post-info"><a class="article-title" href="/2022/01/23/%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E7%AE%97%E6%B3%95/" title="策略梯度算法">策略梯度算法</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2022-01-23T07:52:56.000Z" title="Created 2022-01-23 15:52:56">2022-01-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-David-Silver/">强化学习-David Silver</a></span></div><div class="content">

策略梯度算法

Introduction

策略目标函数


有限差分策略梯度
蒙特卡罗策略梯度

softmax 策略
高斯策略


策略梯度定理（Policy Gradient Theorem）

One-Step MDPs
n-Step MDPs
蒙特卡罗策略梯度
Actor-Critic 策略迭代
利用baseline减小方差 – Advantage Function Critic
Critic 使用不同 Time-Scale
Actors 中使用不同 Time-Scale






策略梯度算法
上一章中介绍了值函数的近似，这是一种通过获得v或者q函数从而得到策略的方式。但是对于选择最优策略这个问题来说，更自然的方式是直接求解策略。
这一节实际上对应了免模型控制那一部分，只不过我们这部分介绍用神经网络的方式对函数进行近似，从而求解。而免模型控制则不从神经网络的方式来求解问题，它一般采用适用于离散情况的价值函数，并采用greedy的方式求解最优策略。这一节则是用神经网络的方式对策略函数进行估计。
上节课我们参数化了价值函数：
V(s,w)≈V(s)Q(s,a,w)≈Q(s ...</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/2022/01/21/%E5%80%BC%E5%87%BD%E6%95%B0%E8%BF%91%E4%BC%BC/" title="值函数近似"><img class="post_bg" src="https://s2.loli.net/2022/01/07/WHBD4aVG6fcXmxj.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="值函数近似"></a></div><div class="recent-post-info"><a class="article-title" href="/2022/01/21/%E5%80%BC%E5%87%BD%E6%95%B0%E8%BF%91%E4%BC%BC/" title="值函数近似">值函数近似</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2022-01-21T10:24:55.000Z" title="Created 2022-01-21 18:24:55">2022-01-21</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-David-Silver/">强化学习-David Silver</a></span></div><div class="content">

值函数近似

Incremental methods

Linear Combination
Lookup table 表现为梯度更新的形式
实际中无监督的情况做近似
Monte-Carlo with Value Function Approximation
TD Learning with Value Function Approximation
Action Value Function Approximation
Convergence


Batch Methods

最小二乘法

LSMC
LSTD
LSTD(λ\lambdaλ)
convergence
LSPI


Deep Q-Network(DQN) + experience replay






值函数近似
前面我们介绍了免模型的预测和控制，但它们的求解和更新价值函数实际上都是查表操作。但是这种方式对于状态空间很大和状态空间连续的情况是不可行的，因此我们需要真正学习出一个函数，它的输入是状态，输出是它的价值，函数的学习可以通过神经网络来完成。也就是:
v^(s,w)=v(s)or q^(s,a,w)=q(s, ...</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/2022/01/20/%E5%85%8D%E6%A8%A1%E5%9E%8B%E6%8E%A7%E5%88%B6/" title="免模型控制"><img class="post_bg" src="https://s2.loli.net/2022/01/07/WHBD4aVG6fcXmxj.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="免模型控制"></a></div><div class="recent-post-info"><a class="article-title" href="/2022/01/20/%E5%85%8D%E6%A8%A1%E5%9E%8B%E6%8E%A7%E5%88%B6/" title="免模型控制">免模型控制</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2022-01-20T14:47:48.000Z" title="Created 2022-01-20 22:47:48">2022-01-20</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-David-Silver/">强化学习-David Silver</a></span></div><div class="content">

免模型控制

Generalised Policy Iteration With Monte-Carlo Evaluation (On-Policy)

ε\varepsilonε-greedy exploration
GLIE 蒙特卡罗控制


TD Control (On-Policy)

Sarsa


Off-Policy Learning
总结




免模型控制
免模型控制可以认为是前面介绍的免模型预测的延续。
免模型控制按照环境交互的策略和目标策略是否相同可以将方法分为两种：

On-policy: 环境交互策略和目标策略一致。就像古代皇帝微服私访，亲力亲为。
Off-policy: 环境交互策略和目标策略不同。就像古代皇帝，派多个官员了解民情。或者机器观察人如何和环境交互，从而学习，它不直接从环境中提取经验，而是从与环境交互的智能体提取经验。。

Generalised Policy Iteration With Monte-Carlo Evaluation (On-Policy)
策略迭代之前在动态规划的控制部分就介绍过。但是它是基于MDP的一种方法，也就是它是一 ...</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/2022/01/19/%E5%85%8D%E6%A8%A1%E5%9E%8B%E9%A2%84%E6%B5%8B/" title="免模型预测"><img class="post_bg" src="https://s2.loli.net/2022/01/07/WHBD4aVG6fcXmxj.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="免模型预测"></a></div><div class="recent-post-info"><a class="article-title" href="/2022/01/19/%E5%85%8D%E6%A8%A1%E5%9E%8B%E9%A2%84%E6%B5%8B/" title="免模型预测">免模型预测</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2022-01-19T09:40:16.000Z" title="Created 2022-01-19 17:40:16">2022-01-19</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-David-Silver/">强化学习-David Silver</a></span></div><div class="content">

免模型预测

蒙特卡罗学习

First-Visit Monte-Carlo Policy Evaluation
Every-Visit Monte-Carlo Policy Evaluation
增量形式的蒙特卡罗更新


时序差分学习

TD(0)
TD(λ\lambdaλ)


蒙特卡罗 vs 时序差分




免模型预测
前面介绍的方法假定我们已知状态转移函数和奖励函数，这在许多游戏的场景中是可行的。但是仍然存在部分场景，我们不能了解它的状态转移函数和奖励函数。
蒙特卡罗学习
蒙特卡罗方法是一种免模型的方法，也就是它不需要环境的状态转移函数以及环境的奖励函数。
蒙特卡罗方法利用完整的回合经验来学习，也就是利用采样样本学习。它的核心思想是利用均值估计期望。
由于这是一个预测任务，因此学习目标是学习给定策略的v函数。
我们之前定义的return和状态价值函数：
Gt=Rt+1+γRt+2+γ2Rt+3+...vπ(s)=Eπ[Gt∣St=s]\begin{equation}
\begin{aligned}
G_t &amp;= R_{t + 1} + \gamma R_{t + ...</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/2022/01/18/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/" title="动态规划"><img class="post_bg" src="https://s2.loli.net/2022/01/07/WHBD4aVG6fcXmxj.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="动态规划"></a></div><div class="recent-post-info"><a class="article-title" href="/2022/01/18/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/" title="动态规划">动态规划</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2022-01-18T14:02:36.000Z" title="Created 2022-01-18 22:02:36">2022-01-18</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-David-Silver/">强化学习-David Silver</a></span></div><div class="content">

动态规划

预测

同步备份
异步备份


控制

策略迭代
值迭代
策略迭代 vs 值迭代


总结
技巧




动态规划
动态表示的是一种问题有时间上的特性，问题一步一步的改变。
动态规划的基本思想是将一个大的问题分解成小的问题，再将小问题的答案结合起来得到最终结果。
动态规划用于解决具有以下两个性质的问题：

具有最优化子结构：可以将问题分解成小问题，分别解决从而解决最终问题
相同的子问题会多次出现，结果可以复用

马尔可夫决策过程（MDP）满足以上两个性质。

贝尔曼方程将问题递归的分解成子问题
价值函数存储和复用子问题的结论，从而获得最终价值函数

动态规划假定可以了解完整的环境信息，也就是假定它知道状态转移概率和奖励函数。也就是动态规划解决的是：如果我们知道状态转移概率和奖励函数，那么我们该如何找到状态价值函数和最优策略
动态规划的过程可以分为两部分：

预测：评估，也就是根据已知策略获得价值函数的过程

输入：

MDP &lt;S,A,P,R,γ&gt;&lt;S, A, P, R, \gamma&gt;&lt;S,A,P,R,γ&gt; 和策略 π\piπ
MR ...</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/2022/01/17/%E6%9C%89%E9%99%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/" title="有限马尔可夫决策过程"><img class="post_bg" src="https://s2.loli.net/2022/01/07/WHBD4aVG6fcXmxj.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="有限马尔可夫决策过程"></a></div><div class="recent-post-info"><a class="article-title" href="/2022/01/17/%E6%9C%89%E9%99%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/" title="有限马尔可夫决策过程">有限马尔可夫决策过程</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2022-01-17T15:17:28.000Z" title="Created 2022-01-17 23:17:28">2022-01-17</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-David-Silver/">强化学习-David Silver</a></span></div><div class="content">

马尔可夫决策过程

马尔可夫决策过程（MDP）的介绍

马尔可夫过程
马尔可夫奖励过程
马尔可夫决策过程






马尔可夫决策过程
一个Agent包含3部分：

策略：action，是agent的behavior。目标
价值函数：评估。对未来做出估计，找到更好的策略
模型：agent对env的表示。根据模型做出计划。包含两部分：

转移概率
reward函数



根据有无价值函数可以将方法分为：策略梯度方法，基于价值函数的方法，actor-critic
根据有无模型可以将方法分为：有模型和无模型
预测：评估策略的价值
控制：寻找最优策略
马尔可夫决策过程（MDP）的介绍
MDP假设环境状态完全可观测。但是部分可观测问题可以转换成为MDP问题来解决。
马尔可夫过程
马尔可夫是一个无记忆的随机序列。它的状态满足马尔可夫性质。马尔可夫过程可以表示为&lt;S,P&gt;&lt;S, P&gt;&lt;S,P&gt;. S为状态集合，P为转移概率。
马尔可夫奖励过程
马尔可夫奖励过程是带有reward的马尔可夫过程。可以表示为&lt;S,P,R,γ&gt;&lt;S, P, R, \ ...</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/2022/01/17/%E6%A6%82%E8%A7%88/" title="概览"><img class="post_bg" src="https://s2.loli.net/2022/01/07/WHBD4aVG6fcXmxj.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="概览"></a></div><div class="recent-post-info"><a class="article-title" href="/2022/01/17/%E6%A6%82%E8%A7%88/" title="概览">概览</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2022-01-17T03:47:25.000Z" title="Created 2022-01-17 11:47:25">2022-01-17</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%A6%82%E8%A7%88/">概览</a></span></div><div class="content">EasyRL: 停止更新。
</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/2022/01/17/%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6/" title="策略梯度"><img class="post_bg" src="https://s2.loli.net/2022/01/07/WHBD4aVG6fcXmxj.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="策略梯度"></a></div><div class="recent-post-info"><a class="article-title" href="/2022/01/17/%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6/" title="策略梯度">策略梯度</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2022-01-16T16:18:04.000Z" title="Created 2022-01-17 00:18:04">2022-01-17</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/EasyRL/">EasyRL</a></span></div><div class="content">

策略梯度

技巧

添加基线
分配合适的分数


REINFORCE：蒙特卡罗策略




策略梯度
强化学习有3个组成部分：演员，环境，奖励
一轮游戏可以叫做一个回合（episode）或者试验（trial）
游戏最终的总奖励（total reward）
一个完整的轨迹可以写成：T={s1,a1,s2,a2,...,st,at}T = \{s_1, a_1, s_2, a_2, ..., s_t, a_t\}T={s1​,a1​,s2​,a2​,...,st​,at​}
该轨迹的概率：
pθ(τ)=p(s1)pθ(a1∣s1)p(s2∣s1,a1)pθ(a2∣s2)...=p(s1)∏t=1Tpθ(at∣st)p(st+1∣st,at)\begin{equation}
\begin{aligned}
p_{\theta}(\tau) 
&amp;= p(s_1)p_{\theta}(a_1|s_1)p(s_2|s_1, a_1)p_{\theta}(a_2|s_2)...\\
&amp;= p(s_1)\prod\limits_{t = 1}^T p_{\theta}(a_t|s_ ...</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/2022/01/16/Q%E8%A1%A8%E6%A0%BC/" title="Q表格"><img class="post_bg" src="https://s2.loli.net/2022/01/07/WHBD4aVG6fcXmxj.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Q表格"></a></div><div class="recent-post-info"><a class="article-title" href="/2022/01/16/Q%E8%A1%A8%E6%A0%BC/" title="Q表格">Q表格</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2022-01-15T16:52:24.000Z" title="Created 2022-01-16 00:52:24">2022-01-16</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/EasyRL/">EasyRL</a></span></div><div class="content">

Q表格

免模型预测

蒙特卡罗策略评估
时序差分
动态规划、蒙特卡罗以及时序差分


免模型控制

探索性初始化蒙特卡罗法
ε\varepsilonε-贪心蒙特卡罗算法
Sarsa：同策略时序差分控制
Q-learning：异策略时序差分控制
同策略 vs 异策略


总结




Q表格
上一章的马尔可夫决策过程说的是有模型情况下的预测和控制。这一章节的Q表格则是介绍了免模型情况下的预测和控制方法。
免模型预测
免模型说的是，不对环境的转移概率和奖励函数进行学习。
蒙特卡罗策略评估
上一章也简单介绍了蒙特卡罗策略评估的方法。简单来说就是对轨迹采样取平均作为期望值。下面会具体介绍。
一条轨迹的回报：
Gt=Rt+1+γRt+2+γ2Rt+3+...G_t = R_{t + 1} + \gamma R_{t + 2} + \gamma^2 R_{t + 3} + ...
Gt​=Rt+1​+γRt+2​+γ2Rt+3​+...
状态价值是s状态以后的轨迹回报的期望：
vT(s)=Eτ∼π[Gt∣st=s]v^T(s) = E_{\tau \sim \pi}[G_t|s_t = s] ...</div></div></div><div class="recent-post-item"><div class="post_cover right"><a href="/2022/01/14/%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/" title="马尔可夫决策过程"><img class="post_bg" src="https://s2.loli.net/2022/01/07/WHBD4aVG6fcXmxj.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="马尔可夫决策过程"></a></div><div class="recent-post-info"><a class="article-title" href="/2022/01/14/%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/" title="马尔可夫决策过程">马尔可夫决策过程</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2022-01-13T17:44:00.000Z" title="Created 2022-01-14 01:44:00">2022-01-14</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/EasyRL/">EasyRL</a></span></div><div class="content">

马尔可夫决策过程

马尔可夫过程
马尔可夫奖励过程

回报与价值函数
V函数的贝尔曼方程
计算马尔可夫奖励过程价值的迭代算法


马尔可夫决策过程

马尔可夫决策过程中的策略
马尔可夫决策过程中的价值函数
Q函数的贝尔曼方程
备份图
预测与控制
动态规划解决马尔可夫决策过程
马尔可夫决策过程的策略评估
马尔可夫决策过程控制






概念：
范围（horizon）：一个回合的长度
回报（return）：奖励折扣后获得的收益
状态价值函数：回报的期望
马尔可夫决策过程
强化学习的过程可以用马尔可夫过程来描述，因为当前状态只和前一个状态以及采取的action有关。马尔可夫决策过程 是强化学习的一个基本框架。
首先介绍马尔可夫奖励过程，它是马尔可夫决策过程的一个简化版本。
接下来会介绍马尔可夫决策过程中的策略评估，给定一个决策，怎么计算它的价值。
最后会介绍马尔可夫决策过程中的决策控制，包含策略迭代和价值迭代两种方式。
马尔可夫过程
马尔可夫性：
当前状态只依赖于前一个状态。
马尔可夫过程/马尔可夫链
可以用状态转移矩阵描述，略。
马尔可夫奖励过程
Markov reward proc ...</div></div></div><nav id="pagination"><div class="pagination"><span class="page-number current">1</span><a class="page-number" href="/page/2/#content-inner">2</a><a class="extend next" rel="next" href="/page/2/#content-inner"><i class="fas fa-chevron-right fa-fw"></i></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Han Yu</div><div class="author-info__description"></div></div><div class="card-info-data is-center"><div class="card-info-data-item"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">12</div></a></div><div class="card-info-data-item"><a href="/tags/"><div class="headline">Tags</div><div class="length-num">3</div></a></div><div class="card-info-data-item"><a href="/categories/"><div class="headline">Categories</div><div class="length-num">4</div></a></div></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>Announcement</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2022/01/23/%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E7%AE%97%E6%B3%95/" title="策略梯度算法"><img src="https://s2.loli.net/2022/01/07/WHBD4aVG6fcXmxj.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="策略梯度算法"/></a><div class="content"><a class="title" href="/2022/01/23/%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E7%AE%97%E6%B3%95/" title="策略梯度算法">策略梯度算法</a><time datetime="2022-01-23T07:52:56.000Z" title="Created 2022-01-23 15:52:56">2022-01-23</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/01/21/%E5%80%BC%E5%87%BD%E6%95%B0%E8%BF%91%E4%BC%BC/" title="值函数近似"><img src="https://s2.loli.net/2022/01/07/WHBD4aVG6fcXmxj.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="值函数近似"/></a><div class="content"><a class="title" href="/2022/01/21/%E5%80%BC%E5%87%BD%E6%95%B0%E8%BF%91%E4%BC%BC/" title="值函数近似">值函数近似</a><time datetime="2022-01-21T10:24:55.000Z" title="Created 2022-01-21 18:24:55">2022-01-21</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/01/20/%E5%85%8D%E6%A8%A1%E5%9E%8B%E6%8E%A7%E5%88%B6/" title="免模型控制"><img src="https://s2.loli.net/2022/01/07/WHBD4aVG6fcXmxj.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="免模型控制"/></a><div class="content"><a class="title" href="/2022/01/20/%E5%85%8D%E6%A8%A1%E5%9E%8B%E6%8E%A7%E5%88%B6/" title="免模型控制">免模型控制</a><time datetime="2022-01-20T14:47:48.000Z" title="Created 2022-01-20 22:47:48">2022-01-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/01/19/%E5%85%8D%E6%A8%A1%E5%9E%8B%E9%A2%84%E6%B5%8B/" title="免模型预测"><img src="https://s2.loli.net/2022/01/07/WHBD4aVG6fcXmxj.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="免模型预测"/></a><div class="content"><a class="title" href="/2022/01/19/%E5%85%8D%E6%A8%A1%E5%9E%8B%E9%A2%84%E6%B5%8B/" title="免模型预测">免模型预测</a><time datetime="2022-01-19T09:40:16.000Z" title="Created 2022-01-19 17:40:16">2022-01-19</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/01/18/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/" title="动态规划"><img src="https://s2.loli.net/2022/01/07/WHBD4aVG6fcXmxj.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="动态规划"/></a><div class="content"><a class="title" href="/2022/01/18/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/" title="动态规划">动态规划</a><time datetime="2022-01-18T14:02:36.000Z" title="Created 2022-01-18 22:02:36">2022-01-18</time></div></div></div></div><div class="card-widget card-categories"><div class="item-headline">
            <i class="fas fa-folder-open"></i>
            <span>Categories</span>
            
            </div>
            <ul class="card-category-list" id="aside-cat-list">
            <li class="card-category-list-item "><a class="card-category-list-link" href="/categories/BasicModels/"><span class="card-category-list-name">BasicModels</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/EasyRL/"><span class="card-category-list-name">EasyRL</span><span class="card-category-list-count">4</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-David-Silver/"><span class="card-category-list-name">强化学习-David Silver</span><span class="card-category-list-count">6</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/%E6%A6%82%E8%A7%88/"><span class="card-category-list-name">概览</span><span class="card-category-list-count">1</span></a></li>
            </ul></div><div class="card-widget card-tags"><div class="item-headline"><i class="fas fa-tags"></i><span>Tags</span></div><div class="card-tag-cloud"><a href="/tags/basic/" style="font-size: 1.1em; color: #999">basic</a> <a href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" style="font-size: 1.5em; color: #99a9bf">强化学习</a> <a href="/tags/%E6%A6%82%E8%A7%88/" style="font-size: 1.1em; color: #999">概览</a></div></div><div class="card-widget card-archives"><div class="item-headline"><i class="fas fa-archive"></i><span>Archives</span></div><ul class="card-archive-list"><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2022/01/"><span class="card-archive-list-date">January 2022</span><span class="card-archive-list-count">12</span></a></li></ul></div><div class="card-widget card-webinfo"><div class="item-headline"><i class="fas fa-chart-line"></i><span>Info</span></div><div class="webinfo"><div class="webinfo-item"><div class="item-name">Article :</div><div class="item-count">12</div></div><div class="webinfo-item"><div class="item-name">UV :</div><div class="item-count" id="busuanzi_value_site_uv"></div></div><div class="webinfo-item"><div class="item-name">PV :</div><div class="item-count" id="busuanzi_value_site_pv"></div></div><div class="webinfo-item"><div class="item-name">Last Push :</div><div class="item-count" id="last-push-date" data-lastPushDate="2022-01-23T07:57:46.390Z"></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2022 By Han Yu</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>